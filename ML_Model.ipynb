{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04961652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense , Dropout\n",
    "import os, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "991a30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 200\n",
    "# Step 1 - Building the CNN\n",
    "\n",
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# First convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape=(sz, sz, 1), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# third convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Flattening the layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding a fully connected layer\n",
    "#units changed 128 to 400 - 500 - 200\n",
    "classifier.add(Dense(units=200, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "#units changed 96 to 800 - 300 - 150\n",
    "classifier.add(Dense(units=150, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=64, activation='relu'))\n",
    "classifier.add(Dense(units=26, activation='softmax')) # softmax for more than 2\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy for more than 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e91e09ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 198, 198, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 99, 99, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 97, 97, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 48, 48, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 46, 46, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 23, 23, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16928)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 200)               3385800   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 150)               30150     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 150)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                9664      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 26)                1690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,446,120\n",
      "Trainable params: 3,446,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - Preparing the train/test data and training the model\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85770880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=10, # rotate the image 30 degrees\n",
    "        width_shift_range=0.1, # Shift the pic width by a max of 10%\n",
    "        height_shift_range=0.1, # Shift the pic height by a max of 10%\n",
    "        rescale=1/255, # Rescale the image by normalzing it.\n",
    "        shear_range=0.2, # Shear means cutting away part of the image (max 20%)\n",
    "        zoom_range=0.2, # Zoom in by 20% max\n",
    "        horizontal_flip=True, # Allo horizontal flipping\n",
    "        fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n",
    "        )\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  shear_range=0.15,\n",
    "                                  zoom_range=0.1,\n",
    "                                  width_shift_range=0.15,\n",
    "                                  height_shift_range=0.1,\n",
    "                                  fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58f031a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13000 images belonging to 26 classes.\n",
      "Found 5200 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('./ProcessedData/train',\n",
    "                                                 target_size=(sz, sz),\n",
    "                                                 batch_size=10,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('./ProcessedData/test',\n",
    "                                            target_size=(sz , sz),\n",
    "                                            batch_size=10,\n",
    "                                            color_mode='grayscale',\n",
    "                                            class_mode='categorical') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4ab8c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 1,\n",
       " 'C': 2,\n",
       " 'D': 3,\n",
       " 'E': 4,\n",
       " 'F': 5,\n",
       " 'G': 6,\n",
       " 'H': 7,\n",
       " 'I': 8,\n",
       " 'J': 9,\n",
       " 'K': 10,\n",
       " 'L': 11,\n",
       " 'M': 12,\n",
       " 'N': 13,\n",
       " 'O': 14,\n",
       " 'P': 15,\n",
       " 'Q': 16,\n",
       " 'R': 17,\n",
       " 'S': 18,\n",
       " 'T': 19,\n",
       " 'U': 20,\n",
       " 'V': 21,\n",
       " 'W': 22,\n",
       " 'X': 23,\n",
       " 'Y': 24,\n",
       " 'Z': 25}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40339f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 1,\n",
       " 'C': 2,\n",
       " 'D': 3,\n",
       " 'E': 4,\n",
       " 'F': 5,\n",
       " 'G': 6,\n",
       " 'H': 7,\n",
       " 'I': 8,\n",
       " 'J': 9,\n",
       " 'K': 10,\n",
       " 'L': 11,\n",
       " 'M': 12,\n",
       " 'N': 13,\n",
       " 'O': 14,\n",
       " 'P': 15,\n",
       " 'Q': 16,\n",
       " 'R': 17,\n",
       " 'S': 18,\n",
       " 'T': 19,\n",
       " 'U': 20,\n",
       " 'V': 21,\n",
       " 'W': 22,\n",
       " 'X': 23,\n",
       " 'Y': 24,\n",
       " 'Z': 25}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af3fda0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "449/449 [==============================] - 104s 232ms/step - loss: 2.4120 - accuracy: 0.2708 - val_loss: 1.0723 - val_accuracy: 0.6779\n",
      "Epoch 2/10\n",
      "449/449 [==============================] - 104s 232ms/step - loss: 0.9897 - accuracy: 0.6728 - val_loss: 0.5101 - val_accuracy: 0.8389\n",
      "Epoch 3/10\n",
      "449/449 [==============================] - 105s 233ms/step - loss: 0.6347 - accuracy: 0.7837 - val_loss: 0.3672 - val_accuracy: 0.8900\n",
      "Epoch 4/10\n",
      "449/449 [==============================] - 104s 232ms/step - loss: 0.4692 - accuracy: 0.8403 - val_loss: 0.2700 - val_accuracy: 0.9116\n",
      "Epoch 5/10\n",
      "449/449 [==============================] - 107s 237ms/step - loss: 0.3833 - accuracy: 0.8717 - val_loss: 0.2697 - val_accuracy: 0.9216\n",
      "Epoch 6/10\n",
      "449/449 [==============================] - 120s 268ms/step - loss: 0.3162 - accuracy: 0.8904 - val_loss: 0.2517 - val_accuracy: 0.9358\n",
      "Epoch 7/10\n",
      "449/449 [==============================] - 108s 241ms/step - loss: 0.2656 - accuracy: 0.9143 - val_loss: 0.2471 - val_accuracy: 0.9316\n",
      "Epoch 8/10\n",
      "449/449 [==============================] - 107s 238ms/step - loss: 0.2512 - accuracy: 0.9187 - val_loss: 0.1997 - val_accuracy: 0.9400\n",
      "Epoch 9/10\n",
      "449/449 [==============================] - 105s 235ms/step - loss: 0.1959 - accuracy: 0.9352 - val_loss: 0.1602 - val_accuracy: 0.9553\n",
      "Epoch 10/10\n",
      "449/449 [==============================] - 113s 251ms/step - loss: 0.2022 - accuracy: 0.9359 - val_loss: 0.1557 - val_accuracy: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1148284b940>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(\n",
    "        training_set,\n",
    "        steps_per_epoch=449, \n",
    "        epochs=10,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a816059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#Importing testing image\n",
    "test_file = './ProcessedData/Test/A/90.jpg'\n",
    "test_img = image.load_img(test_file,target_size=(200, 200))\n",
    "test_img = image.img_to_array(test_img)\n",
    "test_img = cv2.cvtColor(np.array(test_img),cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e39d378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42239362",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = np.expand_dims(test_img, axis=0)\n",
    "test_img = test_img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fd9a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_prob = classifier.predict(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8222ab08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99991417e-01, 1.79025143e-12, 2.81640519e-06, 5.67303232e-07,\n",
       "        1.28991115e-14, 1.53844223e-14, 7.08355033e-13, 4.97730654e-13,\n",
       "        4.67223428e-13, 2.47567857e-18, 2.38973077e-11, 1.37081169e-13,\n",
       "        1.16473443e-12, 3.56667109e-14, 3.60343421e-07, 3.24990174e-10,\n",
       "        1.91790452e-14, 9.53258219e-12, 5.53291898e-07, 4.34579852e-06,\n",
       "        2.26282690e-08, 1.50572541e-10, 3.17505072e-12, 1.12655163e-10,\n",
       "        2.83158474e-09, 5.51347332e-14]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93b8babc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97e64488",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(string.ascii_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43dcfcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[np.argmax(prediction_prob)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eecb2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "import os\n",
    "if not os.path.exists(\"./Model\"):\n",
    "    os.mkdir(\"./Model\")\n",
    "classifier.save(\"./Model/Sign_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae4d6acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json model saved\n",
      "Weights saved\n"
     ]
    }
   ],
   "source": [
    "classifier_Json = classifier.to_json()\n",
    "with open(\"./Model/classifier_json.json\", \"w\") as json_file:\n",
    "    json_file.write(classifier_Json)\n",
    "print('json model saved')\n",
    "classifier.save_weights('./Model/classifier_weights.h5')\n",
    "print('Weights saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e13cbd",
   "metadata": {},
   "source": [
    "# Classifier Building - Completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
